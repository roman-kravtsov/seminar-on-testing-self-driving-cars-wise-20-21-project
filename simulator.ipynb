{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model Detector dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(model_name, model_date):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
    "                                        origin=base_url + model_date + '/' + model_file,\n",
    "                                        untar=True)\n",
    "    return str(model_dir)\n",
    "\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'centernet_hg104_1024x1024_coco17_tpu-32'\n",
    "# PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)\n",
    "# PATH_TO_MODEL_DIR='C:\\\\Users\\\\Roman\\\\.keras\\\\datasets\\\\centernet_hg104_1024x1024_coco17_tpu-32'\n",
    "PATH_TO_MODEL_DIR='E:\\\\Study\\\\Simulation-Based_Testing_Autonomous_Cars\\\\seminar-on-testing-self-driving-cars-wise-20-21-project\\\\exported-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download labels file\n",
    "def download_labels(filename):\n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
    "                                        origin=base_url + filename,\n",
    "                                        untar=False)\n",
    "    label_dir = pathlib.Path(label_dir)\n",
    "    return str(label_dir)\n",
    "\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "# PATH_TO_LABELS = download_labels(LABEL_FILENAME)\n",
    "# PATH_TO_LABELS='C:\\\\Users\\\\Roman\\\\.keras\\\\datasets\\\\mscoco_label_map.pbtxt'\n",
    "PATH_TO_LABELS='E:\\\\Study\\\\Simulation-Based_Testing_Autonomous_Cars\\\\seminar-on-testing-self-driving-cars-wise-20-21-project\\\\exported-models\\\\label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done! Took 28.654932022094727 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL);\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load label map data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(image, image_detections):\n",
    "    \n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # input_tensor = np.expand_dims(image_np, 0)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                   for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    image_np_with_detections = image_np.copy()\n",
    "    return { 'base_image': image_np, 'image_with_detections': image_np_with_detections, 'data': detections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_thresh = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detected_object(image_np_with_detections, detections, category_index, min_score_thresh = score_thresh):\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections,\n",
    "      detections['detection_boxes'],\n",
    "      detections['detection_classes'],\n",
    "      detections['detection_scores'],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=min_score_thresh,\n",
    "      agnostic_mode=False)\n",
    "    plt.figure()\n",
    "    plt.imshow(image_np_with_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_classes(scores, classes, boxes, min_score_thresh = score_thresh, max_boxes_to_draw = 20):\n",
    "    predicted_classes=[]\n",
    "    for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "        if scores is None or scores[i] > min_score_thresh:\n",
    "            if classes[i] in category_index.keys():\n",
    "                class_name = category_index[classes[i]]['name']\n",
    "                predicted_classes.append(class_name)\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define BEAMNG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_driving_command(vehicle, command):\n",
    "    if (command == 'stop'):\n",
    "        vehicle.ai_set_mode('stopping')\n",
    "        vehicle.ai_set_speed(0.1)\n",
    "        return True\n",
    "        #vehicle.control(throttle = 0, brake = 0)\n",
    "    if (command == 'go'):\n",
    "        vehicle.ai_set_mode('random')\n",
    "        vehicle.ai_set_speed(15)\n",
    "        return True\n",
    "    if (command == 'continue'):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_control_command(traffic_light_color):\n",
    "    if (traffic_light_color == 'red'):\n",
    "        return 'stop'\n",
    "    if (traffic_light_color == 'green'):\n",
    "        return 'go'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traffic_light_color(detection):\n",
    "    detected_classes = get_detected_classes(\n",
    "        detection['data']['detection_scores'],\n",
    "        detection['data']['detection_classes'], \n",
    "        detection['data']['detection_boxes'])\n",
    "    if (len(detected_classes) > 0):\n",
    "        traffic_light_color = detected_classes[0]\n",
    "        return traffic_light_color\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driving_instruction(vehicle, detection, traffic_light_color):\n",
    "    return generate_control_command(traffic_light_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_setup(scenario_folder = 'urban', scenario_name = 'urban_3612'):\n",
    "    beamng = BeamNGpy('localhost', 64256, home='E:/BeamNG.research')\n",
    "    scenario = Scenario(scenario_folder, scenario_name)\n",
    "    scenario.path=Path(join(os.environ['BNG_HOME'], \"levels\", scenario_folder, \"scenarios\"))\n",
    "    return beamng, scenario;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_beamng(beamng, scenario, pos=(3.0,-5.0,0)):\n",
    "    bng = beamng.open(launch=True)\n",
    "    vehicle = Vehicle('ego', model='etk800', licence='RED', color='Red')\n",
    "    timer = Timer()\n",
    "    pos = (-0.3, 1, 1.0)\n",
    "    direction = (0, 1, 0)\n",
    "    fov = 90\n",
    "    resolution = (2048, 2048)\n",
    "    front_camera = Camera(pos, direction, fov, resolution,\n",
    "                              colour=True, depth=True, annotation=True)\n",
    "    vehicle.attach_sensor('front_camera', front_camera)\n",
    "    bng.load_scenario(scenario)\n",
    "    bng.spawn_vehicle(vehicle, pos=pos, rot=None, rot_quat=(0, 0, 0, 0))\n",
    "    vehicle.ai_set_speed(15)\n",
    "    return bng, vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_bng(bng):\n",
    "    bng.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps(steps = 60):\n",
    "    bng.step(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(bng, vehicle):\n",
    "    sensors = bng.poll_sensors(vehicle)\n",
    "    image = sensors['front_camera']['colour'].convert('RGB')\n",
    "    return image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration(bng, vehicle, object_coordinates, passing_traffic_light, last_known_color, violations, images = [],steps = 30):\n",
    "    image = get_image(bng, vehicle)\n",
    "    vehicle.update_vehicle()\n",
    "    vehicle_position = vehicle.state['pos']\n",
    "    traffic_light_color = None\n",
    "    _, min_distance_pair_point_2, min_distance, index_of_nearest_point = get_closest_pair_of_points([vehicle_position], object_coordinates)\n",
    "    if (min_distance < 7 and passing_traffic_light == False):\n",
    "        passing_traffic_light = True\n",
    "    elif (min_distance > 5 and passing_traffic_light == True):\n",
    "        passing_traffic_light = False\n",
    "        \n",
    "        if (last_known_color == \"red\"):\n",
    "            detection = detect_image(image, [])\n",
    "            traffic_light_color = get_traffic_light_color(detection)\n",
    "            violations.append({'object': min_distance_pair_point_2, 'vehicle_position': vehicle_position, 'distance': min_distance, 'last_known_color': last_known_color, 'step_index': len(images) - 1})\n",
    "            last_known_color = None\n",
    "            run_steps(steps)\n",
    "            return passing_traffic_light, last_known_color, violations, images\n",
    "    if (traffic_light_color == None):\n",
    "        detection = detect_image(image, [])\n",
    "        traffic_light_color = get_traffic_light_color(detection)\n",
    "    if (traffic_light_color != None):\n",
    "        last_known_color = traffic_light_color\n",
    "#     driving_instruction = get_driving_instruction(vehicle, detection)\n",
    "#     print(driving_instruction)\n",
    "#     handle_driving_command(vehicle, driving_instruction)\n",
    "    images.append(image)\n",
    "    run_steps(steps)\n",
    "    return passing_traffic_light, last_known_color, violations, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(position):\n",
    "    return {'x': position[0],'y': position[1], 'z': position[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(object_position_1, object_position_2):\n",
    "    return np.linalg.norm(object_position_1-object_position_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_node(coordinates, previous_time):\n",
    "     return {\n",
    "            'x': coordinates['x'],\n",
    "            'y':  coordinates['y'],\n",
    "            'z': orig[2],\n",
    "            #  Calculate timestamps for each node such that the speed between\n",
    "            #  points has a sinusoidal variance to it.\n",
    "            't': previous_time + 10,\n",
    "            }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_node_to_np_array(node):\n",
    "    return np.array([node['x'], node['y'], node['z']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import numpy as np\n",
    "\n",
    "def get_closest_pair_of_points(point_list_1,\n",
    "                           point_list_2):\n",
    "    \"\"\"\n",
    "    Determine the two points from two disjoint lists of points that are closest to \n",
    "    each other and the distance between them.\n",
    "\n",
    "    Args:\n",
    "        point_list_1: First list of points.\n",
    "        point_list_2: Second list of points.\n",
    "\n",
    "    Returns:\n",
    "        Two points that make the closest distance and the distance between them.\n",
    "    \"\"\"\n",
    "    indeces_of_closest_point_in_list_2, distances = pairwise_distances_argmin_min(point_list_1, point_list_2)\n",
    "\n",
    "    # Get index of a point pair that makes the smallest distance.\n",
    "    min_distance_pair_index = np.argmin(distances)\n",
    "\n",
    "    # Get the two points that make this smallest distance.\n",
    "    min_distance_pair_point_1 = point_list_1[min_distance_pair_index]\n",
    "    min_distance_pair_point_2 = point_list_2[indeces_of_closest_point_in_list_2[min_distance_pair_index]]\n",
    "\n",
    "    min_distance = distances[min_distance_pair_index]\n",
    "\n",
    "    return min_distance_pair_point_1, min_distance_pair_point_2, min_distance, indeces_of_closest_point_in_list_2[min_distance_pair_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BEAMNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import mmap\n",
    "import random\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from beamngpy import BeamNGpy, Scenario, Vehicle, setup_logging\n",
    "from beamngpy.sensors import Camera, Timer, Lidar,Electrics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "beamng, scenario = create_setup(scenario_folder = 'urban',scenario_name = 'urban_3620')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beamng, scenario = create_setup(scenario_folder = 'west_coast_usa', scenario_name='lidar_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bng, vehicle = run_beamng(beamng, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bng.start_scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bng.pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_known_color = None\n",
    "passing_traffic_light = False\n",
    "violations = []\n",
    "images = []\n",
    "detections = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSStatic [7486:poledouble0] @ (190.96, 15.36,  0.00)\n",
      "TSStatic [7496:polesingle_1] @ (200.80, 35.18,  0.00)\n",
      "TSStatic [7500:poledouble2] @ (211.12, 29.66,  0.00)\n",
      "TSStatic [7505:poledouble3] @ (206.71, 10.11,  0.00)\n",
      "TSStatic [7509:poledouble4] @ (261.18, 12.18,  0.00)\n",
      "TSStatic [7513:poledouble5] @ (261.03, 34.67,  0.00)\n",
      "TSStatic [7517:poledouble6] @ (276.42, 35.30,  0.00)\n",
      "TSStatic [7521:poledouble7] @ (277.81, 14.82,  0.00)\n",
      "TSStatic [7525:poledouble8] @ (344.60,  1.66,  0.00)\n",
      "TSStatic [7529:poledouble9] @ (352.13, 21.49,  0.00)\n",
      "TSStatic [7533:poledouble10] @ (363.39, 19.41,  0.00)\n",
      "TSStatic [7538:polesingle_11] @ (356.12, -1.87,  0.00)\n"
     ]
    }
   ],
   "source": [
    "object_coordinates = np.array([])\n",
    "for static_object in scenario.find_static_objects():\n",
    "    coordinates = get_coordinates(static_object.pos)\n",
    "    if (coordinates['x'] != 0 and coordinates['y'] != 0 and coordinates['z'] == 0):\n",
    "        print(static_object)\n",
    "        np_node = [transform_node_to_np_array(coordinates)]\n",
    "        if (len(object_coordinates) == 0):\n",
    "            object_coordinates = np_node\n",
    "        else:\n",
    "            object_coordinates = np.concatenate((object_coordinates,np_node),axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_iteration(bng, vehicle, object_coordinates, passing_traffic_light, last_known_color, violations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing_traffic_light, last_known_color, violations = run_iteration(bng, vehicle, object_coordinates, passing_traffic_light, last_known_color, violations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.ai_set_mode(\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(45):\n",
    "    passing_traffic_light, last_known_color, violations, images = run_iteration(bng, vehicle, object_coordinates, passing_traffic_light, last_known_color, violations, images, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'object': array([356.11929321,  -1.8700968 ,   0.        ]),\n",
       "  'vehicle_position': [358.96234130859375,\n",
       "   2.5035128593444824,\n",
       "   0.19839653372764587],\n",
       "  'distance': 5.220224624967022,\n",
       "  'last_known_color': 'red',\n",
       "  'step_index': 33}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(violations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bng.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
    "\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "for violation in violations:\n",
    "    image_index = violation['step_index']\n",
    "    image = images[image_index - 1] # get the last image before the violation happened\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image_np)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    # input_tensor = np.expand_dims(image_np, 0)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                   for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'],\n",
    "          detections['detection_classes'],\n",
    "          detections['detection_scores'],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=20,\n",
    "          min_score_thresh=.45,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    plt.figure(figsize = (12,8))\n",
    "    plt.imshow(image_np_with_detections)\n",
    "    print('Done')\n",
    "plt.show()\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    bng.step(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.ai_set_mode(\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bng.set_relative_camera(pos = (-0.3, 1, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = get_image(bng,vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(\"green-bng-13.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
